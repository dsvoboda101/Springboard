# Hospital Admission Length of Stay Predictor

One of the main features determining the cost of a hospital admission is the length of stay (LoS). Predicting LoS at the time of admission would allow hospitals to more effectively schedule staffing, bed allocation and predict the resources their patients will require.  The model described here could also be used to predict when beds will become available for new or transferred patients or for estimating costs when working with insurance companies. The goal of this project was to build a model to predict LoS in the hospital based on vital signs and medical history.

## Data

We used the LengthOfStay.csv dataset describing 100,000 patients admitted to the hospital. This dataset was generated by Microsoft and has been deidentified, so the hospital and type of ward are unknown. This dataset includes nine medical test results, such as heart rate or blood glucose levels, and twelve features of patient medical history, such as the number of prior ICU admissions or the presence of a psychological disorder.  This dataset if publicly available on Kaggle at `https://www.kaggle.com/datasets/aayushchou/hospital-length-of-stay-dataset-microsoft/data`.

## Data Wrangling

The LengthOfStay (LoS) dataset was relatively clean and required minimal data wrangling to prepare for EDA. The following three issues were addressed: 
1)	**Readmission Count (called rcount) was converted from a category to an integer.** 

Initially, rcount contained the values 0, 1, 2, 3, 4, or 5+. The ‘+’ character prevented this column from being treated as a number.  To address this, the ‘+’ was removed and the column was renamed to ‘readmission_int’.  In addition, a new column was created that kept readmissions as a category with the options ‘None’, 1-2 admissions, 3-4 or 5+, but this column was eventually discarded for redundancy.  

2)	**Date columns were converted to usable formats and investigated but eventually discarded.**  

The dataset included columns for the date admitted (called ‘vdate’) and discharged (called ‘discharged’).  The ‘discharged’ column was removed as it provided no additional information.  We then used the ‘vdate’ column to investigate the possibility that patient admission and length of stay might vary by month. To do this, we extracted the month into a new column called ‘admission_month’ and renamed ‘vdate’ to the more interpretable ‘admission_date’.   The number of patients and average length of stay were then plotted against in month of admission, with no obvious trends detected.

<img src="Reports\images\Figure1.png" width="900">

*Figure 1: Patients admitted per month (left) and mean length of stay by month of admission (right).*
 
3)	**Rows with Blood Urea Nitrogen (BUN) above 300 dg/mL were removed.**
Normal BUN levels range from 6 – 20 mg/dL and values above 50 mg/dL indicating a health problems (https://simplenursing.com/bun-levels/).  Patients in end stage kidney failure can have levels from 100 up to 250 mg/dL.  The LoS dataset included values in the ‘bloodureanitro’ column up to 682 mg/dL.  Because these high values seem above possible limits and suggest an input error, rows with ‘bloodureanitro’ values above 300 mg/dL were removed from the dataset.


## Exploratory Data Analysis

### Test Results and Continuous Features

The first step of EDA is often to determine which features correlate with the outcome under investigation.  In the LoS dataset, we first looked at which numeric features correlated with the admission length and found that the number of prior admissions was the only feature to visibly correlate with admission length (Figure 3). 

<img src="Reports\images\Figure2.png" width="700">

*Figure 2: The number of previous admissions positively correlates with Length of Stay.*

However, we found that many of the numeric variables had the same shape to the data when plotted against length of stay in a scatterplot.   While not a direct correlation, this indicates that extreme values in these tests are associated with shorter admissions lengths (Figure 4).  This information suggests that tree-based models, which can use the middle ranges of test results as decision points, are likely to be successful at modelling this dataset.  The features which shared this common shape are listed below.
•	Hematocrit
•	Neutrophil Count
•	Blood Sodium Concentration
•	Blood Glucose Concentration
•	Creatinine
•	Body Mass Index (BMI)
•	Pulse 
•	Respiration
 
 <img src="Reports\images\Figure3.png" width="900">

*Figure 3: Creatinine levels, BMI and pulse are plotted against admission length as examples of the shape the scatterplot takes when comparing Length of Stay to most numeric features in this dataset.*

### Medical History as Categorical Features

In contrast to the numeric data, many of the medical history features were found to be associated with longer admissions (Figure 4).  While not statistically significant, these trends combine to tell a story that patients with longer admissions have a greater number of health complications.  

 <img src="Reports\images\Figure4.png" width="800">

 *Figure 4: Length of Stay compared to all categorical features in boxplots.
0 = Patient doesn’t have complication.
1=Patient has complication.*


To better capture this trend and assist with model building, we created a new feature to capture this trend call ‘total_issues’ which is the sum of all complications for each patient.  The number of complications was found to be weakly correlated length of stay.

 <img src="Reports\images\Figure5.png" width="500">  

 *Figure 5: Number of complications vs Length of Stay has a weak correlation of 0.471.*


 ## Model Building

Model building proceeded through three stages: 
1)	Algorithm Screening 
2)	Hyperparameter Tuning with the Top Three Learners
3)	Model Refinement


### 1) Algorithm Screening 

Seven regression learners were tested with default settings to determine which was the best starting place for building a model to predict the length of ICU stay using the LengthOfStay dataset.  The models were judged based on their R-squared values and mean absolute error.  The conclusions of these tests are summarized in Table 1.

The learners were also evaluated by comparing the predictions of each patient’s stay to their actual values by scatterplot (Figure 6).  The red line in each plot shows where the data would lie if the predictions and actual values are equal.  In some tests, the predictions appear to get worse for longer admission stays.  To account for this, we added an additional column to Table 1 indicating if the predictions appeared to maintain consistent accuracy at 10+ days. 

<img src="Reports\images\Table1.png" width="600">

Based on these results, the CatBoost Regressor is the best model for moving forward, as it has the lowest error, a high R-squared and maintains accuracy for longer hospital stays. In addition, we will select the XGBoost Regressor and Random Forest Regressor for further development.

 <img src="Reports\images\Figure6.png" width="800">


*Figure 6: Predicted vs Actual values for length of stay for increasingly sophisticated algorithms. The red dotted line shows where the data would lie with perfect predictions. The red circle shows predictions getting less accurate for longer stays with the GB Regressor learner. The green circle shows prediction accuracy staying consistent for longer admissions with the CatBoost learner.*


### 2) Hyperparameter Tuning with the Top Three Learners

Random Forest Regressor, XGBoost Regressor and CatBoost Regressor were built as complete models with hyperparameter optimization. While the details are beyond the scope of this report, hyperparameter tuning led to very modest improvements in the XGBoost and CatBoost MAE while the Random Forest models could not be improved beyond default parameters as summarized in Table 2.  

<img src="Reports\images\Table2.png" width="450">

However, when predicted values for the tuned CatBoost model were compared to actual values by scatterplot we saw the familiar loss of accuracy for longer stays.  

<img src="Reports\images\Figure7.png" width="800">

*Figure 7: Predicted vs Actual values for length of stay for CatBoost Regressor at default (left) versus optimized (right). The green circle shows prediction accuracy staying consistent for longer admissions with the default CatBoost learner. The red circle shows predictions getting less accurate for longer stays with the tuned complete CatBoost model.*

To investigate this further, we calculated the error of each model for patients admitted for less than 10 days separately from patients admitted 10 or more days.  The results are summarized in Figure 8.

<img src="Reports\images\Figure8.png" width="500">

*Figure 8: MAE for stays less than or greater than 10 days. Learners with ‘model’ have been optimized.*

These results show that the default CatBoost learner has the lowest error for admissions of 10 or more days.  In addition, the error of all models on shorter stays is quite consistent at around 0.3 days and not appreciably lowered through model optimization.   This finding is important for model selection, since accurately identifying longer hospital admissions is likely to be important from a business perspective, as these patients would require the most resources and longer bed alocations.  **We  recommend the CatBoost Regressor with default setting as the final model for predicting hospital stay using the LoS dataset.**


### Model Refinement

The final step in preparing this model for a client is to determine how many of these features are necessary for the model to make accurate predictions.  To answer this question, we sorted the features in order of importance and tested models with an increasing number of the most important features.  The error rates of these models are shown in Figure 9.

<img src="Reports\images\Figure9.png" width="500">

*Figure 9: MAE for stays of less than 10 days (blue solid line) vs 10 or more days (orange solid line) from models built with an increasing number of features.  Blue dotted line = MAE of 0.5 day.  Orange dotted line = MAE of 0.28 days.*

These results show that the model requires the top eleven features to predict hospital stays. These features are:  
•	Number of Readmission  
•	Total Health Complications  
•	Hematocrit  
•	BMI  
•	Creatinine  
•	Blood Glucose   
•	Blood Sodium  
•	Pulse  
•	Respiration  
•	Neutrophil Count  
•	Blood Urea Nitrogen  


## Future Work

Predicting the length of a patient’s admission to the hospital at the time of admission would allow hospitals to more effectively schedule staffing, bed allocation and predict the resources their patients will require.  This model could also be used to predict when beds will become available for new or transferred patients or for estimating costs when working with insurance companies.  The above project exemplifies how this model can be built from a single dataset to predict stays of up to 2 weeks with a mean error of plus or minus 0.27 days.

In addition to the medical data, the LoS dataset also contained facility IDs (called ‘facid’) which implies that the patients can from five different hospitals, or at least five wards of a single hospital. The next step that I recommend in processing this dataset is to examine how length of stay and prediction accuracy varies across these different facilities to ensure the model is generalizable across hospitals.  To truly test whether the model can generalize predictions across hospitals, I would recommend testing it on new data collected from the facilities used in the LoS dataset, as well as at least one other hospital.

During EDA, we found that the only piece of medical data that correlated with length of stay was the number of prior readmissions. Beyond that, we also found that there were no significant correlations between the data features. To our human eyes, this initial investigation made the data look unpredictable. This lack of clear patterns makes it difficult for us to intuitively guess which pieces of medical data are most useful in predicting length of stay. Fortunately, there are numerous datasets available designed for predicting patient length of stays with different patient groups. To better understand which pieces of medical data are more effective for predicting admission length, I recommend doing a meta study to compare the types of medical data collected across datasets and comparing the features selected by predictive models trained on these datasets.  This could better inform data collection and allow us to build more accurate and efficient predictive models.

## Credits 

Thank you Aayush Choudhury for posting the LengthOfStay.csv dataset on Kaggle, Govind Malhotra for being a fantastic mentor with Springboard, and the authors of "A systematic review of the prediction of hospital length of stay: Towards a unified framework" which does a fantastic job of showing how analysis like this can be brought to the next level.

Stone K, Zwiggelaar R, Jones P, Mac Parthaláin N. A systematic review of the prediction of hospital length of stay: Towards a unified framework. PLOS Digit Health. 2022 Apr 14;1(4):e0000017. doi: 10.1371/journal.pdig.0000017. PMID: 36812502; PMCID: PMC9931263.

